# üñá Uni-HAR: Universal Human Action Recognition

[![Hugging Face Model](https://img.shields.io/badge/Hugging_Face-model_card-ff69b4)](https://huggingface.co/m97j/uni-har)

**Multimodal Pose + Image Fusion-Based Action Recognition Model**

---

# üìå Project Overview

Uni-HAR is a real-time human action recognition model designed to universally understand the complex dynamic movements of the human body, going beyond simple action classification.

This model uses:

* **Pose sequence (skeleton)**
* **RGB image sequence**

and fuses them using a multi-scale transformer architecture for robust behavior classification.

---

# üéØ Key Features

### 1) **Multimodal Fusion**

* **Pose**: Motion structure, invariant to illumination/background
* **Image**: Context such as objects, environment
* ‚Üí Combining both yields higher robustness and accuracy

### 2) **Efficient Transformer-Based Architecture**

* Temporal/Spatial Factorized Attention (PoseFormerFactorized)
* Reduced complexity from $(O(T^2J^2)) ‚Üí (O(T^2J + J^2T))$

### 3) **Real-time & Scalable**

* Low latency inference
* Modular: Easily extend to IMU, depth sensors, or other modalities

---

# üìÅ Repository Structure

```
uni-har/
‚îÇ‚îÄ‚îÄ models/
‚îÇ   ‚îú‚îÄ‚îÄ pose/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ poseformer_factorized.py
‚îÇ   ‚îú‚îÄ‚îÄ image/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ image_encoder.py
‚îÇ   ‚îú‚îÄ‚îÄ fusion/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ fusion_model.py
‚îÇ   ‚îî‚îÄ‚îÄ multiscale_model.py
‚îÇ
‚îÇ‚îÄ‚îÄ inference/
‚îÇ   ‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ download_model.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ load_config.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ load_labels.py
‚îÇ   ‚îú‚îÄ‚îÄ build_model.py
‚îÇ   ‚îú‚îÄ‚îÄ predictor.py
‚îÇ   ‚îî‚îÄ‚îÄ example_infer.py
‚îÇ
‚îÇ‚îÄ‚îÄ README.md
```

---

# üì¶ Installation

```bash
pip install torch torchvision
pip install timm
pip install huggingface_hub
```

(ResNet-18 is loaded from torchvision; timm optional if you switch backbones.)

HuggingFace dependency:

```
huggingface_hub >= 0.23
```

---

# ‚ö° Device Auto Selection

The predictor automatically selects:

* **CUDA (GPU)** if available
* **CPU** otherwise

You can override manually:

```python
predictor = HARPredictor(weight_path, config, device="cpu")
```

---

# üß© Input Format

The model consumes **two parallel sequences**:

---

## 1) Pose Sequence

**Tensor shape:**

```
(B, C, T, J, 3)
```

Where:

| Dimension | Meaning                          |
| --------- | -------------------------------- |
| B         | batch size                       |
| C         | channels (pose encoding streams) |
| T         | 30 frames                        |
| J         | 17 joints                        |
| 3         | (x, y, confidence)               |

---

## 2) RGB Image Sequence

**Tensor shape:**

```
(B, C, T, 3, 224, 224)
```

* Each frame is resized to **224 √ó 224**
* Uses ResNet-18 backbone (modifiable)

---

# üîç Running Inference

Below is the minimal working example:

```python
import torch

from inference.utils.load_config import load_config
from inference.utils.load_labels import load_labels
from inference.utils.download_model import download_model
from inference.predictor import HARPredictor

cfg = load_config()
labels = load_labels()

predictor = HARPredictor(
    weight_path=download_model(),
    config=cfg
)

# Dummy inputs
pose = torch.randn(1, 4, 30, 17, 3)
img  = torch.randn(1, 4, 30, 3, 224, 224)

topk_ids, topk_probs = predictor.topk(pose, img, k=5)

print("Top-k Predictions:")
for idx, score in zip(topk_ids[0], topk_probs[0]):
    print(labels[str(idx.item())], f"{score.item():.4f}")
```

---

# üìà Output Format ‚Äî Probability Distribution (Not Argmax!)

Unlike standard classifiers, this model produces a **500-dimensional probability vector**.  
This design supports:

---

## ‚úî Top-K Semantic Ranking

```python
topk_ids, topk_probs = predictor.topk(pose, img, k=5)
```

---

## ‚úî Threshold-based Filtering

Useful in ambiguous actions:

```python
filtered = predictor.threshold(pose, img, thr=0.1)
```

---

# üìä Dataset

Training is based on:

### **MPOSE**

* BODY_25 format
* 30-frame pose sequences

### **HAA500**

* RGB videos + synchronized OpenPose skeleton

---

# üèó Model Architecture

* **PoseFormerFactorized**  
  Temporal/Spatial attention separation

* **ImageEncoder (ResNet-18)**  
  Extracts contextual features

* **MultiModalFusionModel**  
  Late fusion + probability output

---

# üöÄ Training Strategy (Overview)

1. **Stage 1 ‚Äî Pose-only pretraining**  
   Train PoseFormerFactorized on MPOSE

2. **Stage 2 ‚Äî Multimodal fine-tuning**  
   Add RGB encoder and fusion module  
   Train on HAA500  

(Training code not included in this repository.)

---

# üìú License

Apache License 2.0

---

